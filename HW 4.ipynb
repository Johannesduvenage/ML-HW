{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML HW4\n",
    "### 106598018 萬俊瑋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. In the SOFM network, the learning rate is a function of time and distance between the current node and the BMU. If η0 = σ0 = 0.1, λ = 10, find the number of required iteration such that the learning rate of nodes next to BMU is less than 0.001. A node is next to BMU is located at (x0 ± 1, y0) or (x0, y0 ±1) where (x0, y0) as the coordinate of the BMU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:1 3.318986956663983e-28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "learningRate = 1\n",
    "t = 1\n",
    "while learningRate >= 0.001:\n",
    "    learningRate = 0.1*np.exp(t/10)*np.exp(-1/(2*(0.1*np.exp(-t/10))**2))\n",
    "    t += 1\n",
    "print('Iteration:' + str(t - 1), learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/wei840222/ML-HW/blob/master/images/HW4Q1.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. We mentioned that the parameter α in GMM was computed based on the Lagrange multipliers. Show that αj = (1/n)∑βj(xi) as given in the PPT notes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[http://www.cs.nccu.edu.tw/~whliao/acv2008/08gmm.pdf](http://www.cs.nccu.edu.tw/~whliao/acv2008/08gmm.pdf)\n",
    "![](https://github.com/wei840222/ML-HW/blob/master/images/HW4Q2.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. We have analytically solved the following problem: Maximize f(x, y) = x + y subject to x2 + y2 = 1. Write a gradient descent program to find the solution numerically. Note that to find the maximum point, you need to follow the gradient (instead of negative gradient). Compare your numerical results with analytical results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 93 The max number of f(x, y) = x + y subject to x^2 + y^2 = 1: 1.414212742590096\n"
     ]
    }
   ],
   "source": [
    "def numericalGradient(F, X):\n",
    "    h = 1e-6\n",
    "    G = np.zeros_like(X)\n",
    "    for i in range(len(X)):\n",
    "        oriX = X[i]\n",
    "        X[i] = oriX + h\n",
    "        FXh1 = F(X)\n",
    "        X[i] = oriX - h\n",
    "        FXh2 = F(X)\n",
    "        G[i] = (FXh1 - FXh2)/(2*h)\n",
    "        X[i] = oriX\n",
    "    return G\n",
    "\n",
    "def gradientDescent(F, X, eta=0.1, maxEpoch=1000):\n",
    "    epoch = 1\n",
    "    while True:\n",
    "        G = numericalGradient(F, X)\n",
    "        X += eta*G\n",
    "        if np.sqrt(np.sum(G**2)) < 1e-6 or epoch >= maxEpoch:\n",
    "            break\n",
    "        epoch += 1\n",
    "    return X, epoch\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    def myLagrangeMutipliers(X):\n",
    "        x = X[0]\n",
    "        y = X[1]\n",
    "        # magic number of my analytical results\n",
    "        l = -1*np.sqrt(2)/2\n",
    "        return x + y + l*(x**2 + y**2 - 1)\n",
    "    \n",
    "    X = np.random.uniform(0., 1., 3)\n",
    "    X, epoch = gradientDescent(myLagrangeMutipliers, X)\n",
    "    \n",
    "    print('epoch:', epoch, 'The max number of f(x, y) = x + y subject to x^2 + y^2 = 1:', X[0] + X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/wei840222/ML-HW/blob/master/images/HW4Q3.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Compute the complete update (back propagation) equations for all weights (w1 ~ w8) in the following neural networks. The activation function is sigmoid, the loss function is MSE, and the desired outputs are d1 and d2.**\n",
    "![](https://github.com/wei840222/ML-HW/blob/master/images/HW4Q4NN.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/wei840222/ML-HW/blob/master/images/HW4Q4.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Write a program to implement the neural network with your back propagation equations in problem 4. To test your network, train it to distinguish the classes of versicolor and virginica in the Iris dataset using only the third and fourth features\n",
    "(i.e., petal length and petal width) as the inputs. As usual, use 70% of the data for training and the rest for testing. Repeat the experiments 10 times to find the average accuracy. During training, set the desired output as 0.9 for in class data\n",
    "and 0.1 for out of class data. Don’t forget to use random numbers as the initial weights.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**numpy version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "class TwoLayerNN:\n",
    "    def __init__(self):\n",
    "        # init weight of Xavier init value\n",
    "        self.W = np.random.randn(8)/np.sqrt(2)\n",
    "        \n",
    "    def predict(self, x1, x2):\n",
    "        w1, w2, w3, w4, w5, w6, w7, w8 = self.W[0], self.W[1], self.W[2], self.W[3], self.W[4], self.W[5], self.W[6], self.W[7]\n",
    "        q1 = w1*x1 + w2*x2\n",
    "        q2 = w3*x1 + w4*x2\n",
    "        h1 = sigmoid(q1)\n",
    "        h2 = sigmoid(q2)\n",
    "        z1 = w5*h1 + w6*h2\n",
    "        z2 = w7*h1 + w8*h2\n",
    "        y1 = sigmoid(z1)\n",
    "        y2 = sigmoid(z2)\n",
    "        return y1, y2\n",
    "    \n",
    "    def loss(self, x1, x2, d1, d2):\n",
    "        y1, y2 = self.predict(x1, x2)\n",
    "        return ((y1 - d1)**2 + (y2 - d2)**2)/2\n",
    "    \n",
    "    def fit(self, X, D, eta = 0.01, epoch = 1000, printIvl = 100):\n",
    "        loss_history = list()\n",
    "        for i in range(epoch):\n",
    "            loss_sum = 0\n",
    "            for x, d in zip(X, D):\n",
    "                loss_W = lambda W:self.loss(x[0], x[1], d[0], d[1])\n",
    "                G = numericalGradient(loss_W, self.W)\n",
    "                self.W -= eta*G\n",
    "                loss_sum += self.loss(x[0], x[1], d[0], d[1])\n",
    "            loss_history.append(loss_sum/len(X))\n",
    "            if (i+1) % printIvl == 0:\n",
    "                print('epoch:' + str(i+1) + ' | loss:' + str(loss_history[i]))\n",
    "        return loss_history\n",
    "    \n",
    "    def accuracy(self, X, D):\n",
    "        acc = 0\n",
    "        for x, d in zip(X, D):\n",
    "            y1, y2 = self.predict(x[0], x[1])\n",
    "            y = np.argmax([y1, y2])\n",
    "            d = np.argmax([d[0], d[1]])\n",
    "            if y ==d:\n",
    "                acc += 1\n",
    "        return acc/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:100 | loss:0.14704557739858318\n",
      "epoch:200 | loss:0.11384473356779293\n",
      "epoch:300 | loss:0.12002858677299637\n",
      "epoch:400 | loss:0.11489568178936213\n",
      "epoch:500 | loss:0.1297567246683789\n",
      "epoch:600 | loss:0.12876861841342568\n",
      "epoch:700 | loss:0.1180325579390185\n",
      "epoch:800 | loss:0.11370644094184891\n",
      "epoch:900 | loss:0.11903254766143946\n",
      "epoch:1000 | loss:0.14030589249383008\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'accuracy' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-912e0abcb210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoLayerNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-2b8d394ae10e>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, X, D)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'accuracy' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make results reproducible\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "dataset = dataset[50:150]\n",
    "dataset = pd.get_dummies(dataset, columns=[4]) # One Hot Encoding\n",
    "values = list(dataset.columns.values)\n",
    "\n",
    "y = dataset[values[-2:]]\n",
    "y = np.array(y, dtype='float32')\n",
    "for i in range(len(y)):\n",
    "    if y[i][0] == 1:\n",
    "        y[i][0] = 0.9\n",
    "        y[i][1] = 0.1\n",
    "    else:\n",
    "        y[i][0] = 0.1\n",
    "        y[i][1] = 0.9\n",
    "X = dataset[values[2:4]]\n",
    "X = np.array(X, dtype='float32')\n",
    "\n",
    "# Shuffle Data\n",
    "indices = np.random.choice(len(X), len(X), replace=False)\n",
    "X_values = X[indices]\n",
    "y_values = y[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 30\n",
    "X_test = X_values[-test_size:]\n",
    "X_train = X_values[:-test_size]\n",
    "y_test = y_values[-test_size:]\n",
    "y_train = y_values[:-test_size]\n",
    "\n",
    "NN = TwoLayerNN()\n",
    "loss = NN.fit(X_train, y_train, 1, 1000)\n",
    "NN.accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 100 | Loss: 11.028534\n",
      "Epoch 200 | Loss: 10.886512\n",
      "Epoch 300 | Loss: 10.7463875\n",
      "Epoch 400 | Loss: 10.542257\n",
      "Epoch 500 | Loss: 10.15292\n",
      "Epoch 600 | Loss: 9.434029\n",
      "Epoch 700 | Loss: 8.301454\n",
      "Epoch 800 | Loss: 6.8987083\n",
      "Epoch 900 | Loss: 5.544415\n",
      "Epoch 1000 | Loss: 4.4695134\n",
      "\n",
      "Testing...\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [1. 0.] Predicted: [0. 1.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [0. 1.] Predicted: [1. 0.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [0. 1.] Predicted: [1. 0.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Actual: [1. 0.] Predicted: [1. 0.]\n",
      "Actual: [0. 1.] Predicted: [0. 1.]\n",
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make results reproducible\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "dataset = dataset[50:150]\n",
    "dataset = pd.get_dummies(dataset, columns=[4]) # One Hot Encoding\n",
    "values = list(dataset.columns.values)\n",
    "\n",
    "y = dataset[values[-2:]]\n",
    "y = np.array(y, dtype='float32')\n",
    "for i in range(len(y)):\n",
    "    if y[i][0] == 1:\n",
    "        y[i][0] = 0.9\n",
    "        y[i][1] = 0.1\n",
    "    else:\n",
    "        y[i][0] = 0.1\n",
    "        y[i][1] = 0.9\n",
    "X = dataset[values[2:4]]\n",
    "X = np.array(X, dtype='float32')\n",
    "\n",
    "# Shuffle Data\n",
    "indices = np.random.choice(len(X), len(X), replace=False)\n",
    "X_values = X[indices]\n",
    "y_values = y[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 30\n",
    "X_test = X_values[-test_size:]\n",
    "X_train = X_values[:-test_size]\n",
    "y_test = y_values[-test_size:]\n",
    "y_train = y_values[:-test_size]\n",
    "\n",
    "# Session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Interval / Epochs\n",
    "interval = 100\n",
    "epoch = 1000\n",
    "\n",
    "# Initialize placeholders\n",
    "X_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "\n",
    "# Input neurons : 2\n",
    "# Hidden neurons : 2\n",
    "# Output neurons : 2\n",
    "hidden_layer_nodes = 2\n",
    "\n",
    "# Create variables for Neural Network layers\n",
    "w1 = tf.Variable(tf.random_normal(shape=[2,hidden_layer_nodes])) # Inputs -> Hidden Layer\n",
    "b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]))   # First Bias\n",
    "w2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes,2])) # Hidden layer -> Outputs\n",
    "b2 = tf.Variable(tf.random_normal(shape=[2]))   # Second Bias\n",
    "\n",
    "# Operations\n",
    "hidden_output = tf.nn.sigmoid(tf.add(tf.matmul(X_data, w1), b1))\n",
    "final_output = tf.nn.sigmoid(tf.add(tf.matmul(hidden_output, w2), b2))\n",
    "\n",
    "# Cost Function\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(final_output - y_target), axis=0))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training\n",
    "print('Training the model...')\n",
    "for i in range(1, (epoch + 1)):\n",
    "    sess.run(optimizer, feed_dict={X_data: X_train, y_target: y_train})\n",
    "    if i % interval == 0:\n",
    "        print('Epoch', i, '|', 'Loss:', sess.run(loss, feed_dict={X_data: X_train, y_target: y_train}))\n",
    "\n",
    "# Prediction\n",
    "accuracy = 0\n",
    "print('\\nTesting...')\n",
    "for i in range(len(X_test)):\n",
    "    t = np.array([0., 0.])\n",
    "    t[np.argmax(y_test[i])] = 1.\n",
    "    y = np.rint(sess.run(final_output, feed_dict={X_data: [X_test[i]]}))[0]\n",
    "    \n",
    "    if t[0] == y[0] and t[1] == y[1]:\n",
    "        accuracy += 1\n",
    "    print('Actual:', t, 'Predicted:', y)\n",
    "print('Accuracy:', accuracy/test_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
