{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML HW4\n",
    "### 106598018 萬俊瑋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. In the SOFM network, the learning rate is a function of time and distance between the current node and the BMU. If η0 = σ0 = 0.1, λ = 10, find the number of required iteration such that the learning rate of nodes next to BMU is less than 0.001. A node is next to BMU is located at (x0 ± 1, y0) or (x0, y0 ±1) where (x0, y0) as the coordinate of the BMU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "learningRate = 1\n",
    "t = 0\n",
    "while learningRate >= 0.001:\n",
    "    learningRate = 0.1*np.exp(t/10)*np.exp(-1/(2*(0.1*np.exp(-t/10))**2))\n",
    "    t += 1\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. We mentioned that the parameter α in GMM was computed based on the Lagrange multipliers. Show that αj = (1/n)∑βj(xi) as given in the PPT notes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[http://www.cs.nccu.edu.tw/~whliao/acv2008/08gmm.pdf](http://www.cs.nccu.edu.tw/~whliao/acv2008/08gmm.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. We have analytically solved the following problem: Maximize f(x, y) = x + y subject to x2 + y2 = 1. Write a gradient descent program to find the solution numerically. Note that to find the maximum point, you need to follow the gradient (instead of negative gradient). Compare your numerical results with analytical results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.970324838183736e+19 5.629499537334133e+18 1.4073748844032555e+19\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def lagrangeMutipliers(x, y, l):\n",
    "    return x + y + l*(x**2 + y**2 - 1)\n",
    "\n",
    "def numericalGradient(f, x, y, l):\n",
    "    h = 10e-6\n",
    "    gx = (f(x+h, y, l) - f(x-h, y, l))/(2*h)\n",
    "    gy = (f(x, y+h, l) - f(x, y-h, l))/(2*h)\n",
    "    gl = (f(x, y, l+h) - f(x, y, l-h))/(2*h)\n",
    "    return gx, gy, gl\n",
    "\n",
    "x = random.randint(0, 100)\n",
    "y = random.randint(0, 100)\n",
    "l = random.randint(0, 100)\n",
    "eta = 0.1\n",
    "G = np.array(numericalGradient(lagrangeMutipliers, x, y, l))\n",
    "\n",
    "while np.sqrt(np.sum(G**2)) > 10e-6:\n",
    "    x += eta*G[0]\n",
    "    y += eta*G[1]\n",
    "    l += eta*G[2]\n",
    "    G = np.array(numericalGradient(lagrangeMutipliers, x, y, l))\n",
    "print(x, y, l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
