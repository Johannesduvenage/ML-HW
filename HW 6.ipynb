{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML HW6\n",
    "### 106598018 萬俊瑋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. We mentioned that the parameters 𝛾 and 𝛽 in batch normalization are trained through backprop. Explain why we don’t want to directly compute these two parameters from the training samples and also give the details how the training is carried out.**  \n",
    ">Beacause the mean and varence of data in each batch are different, so we need to train the 𝛾 and 𝛽 like training the weight, but not compute they from the all data set.  \n",
    "And we can train the 𝛾 and 𝛽 in the same way which we train the weight in NN. We can compute the partial derivative of loss function for 𝛾 and 𝛽 and do the gradient decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. We use the “Reparameterization Trick” in training Variational AutoEncoder (VAE). Explain why this trick is necessary and how it is accomplished.**\n",
    ">The reparameterization trick is as follows. Recall, if we have x∼N(μ,Σ) and then standardize it so that μ=0,Σ=1, we could revert it back to the original distribution by reverting the standardization process. Hence, we have this equation:x=μ+Σ1/2Xstd  \n",
    "With that in mind, we could extend it. If we sample from a standard normal distribution, we could convert it to any Gaussian we want if we know the mean and the variance. Hence we could implement our sampling operation of \n",
    "z by:z=μ(X)+Σ1/2(X)ϵ where ϵ∼N(0,1).  \n",
    "Now, during backpropagation, we don’t care anymore with the sampling process, as it is now outside of the network, i.e. doesn’t depend on anything in the net, hence the gradient won’t flow through it.  \n",
    "However, we are now facing a problem. How do we get z from the encoder outputs? Obviously we could sample z from a Gaussian which parameters are the outputs of the encoder. Alas, sampling directly won’t do, if we want to train VAE with gradient descent as the sampling operation doesn’t have gradient!  \n",
    "There is, however a trick called reparameterization trick, which makes the network differentiable. Reparameterization trick basically divert the non-differentiable operation out of the network, so that, even though we still involve a thing that is non-differentiable, at least it is out of the network, hence the network could still be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Use the equations of optimal margin (linear) SVM (in pp. 12) to find 𝐰 given 𝐱1 =[1 −1]𝑇 ∈𝐶+1 and 𝐱2 =[−1 −1]𝑇 ∈𝐶−1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Implement a discrete HMM training program. Use the three-urn example (in pp.\n",
    "12 of the PPT file) to test your program and produce the training results after 100\n",
    "iterations. Use the red and blue balls in each urn to compute the initial emission\n",
    "111 333\n",
    "probability. The initial transition probability A = 1 1 1 and π = [1 1 1]. 333 333\n",
    "111 [3 3 3]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
